{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this dictionary to map state names to two letter acronyms\n",
    "states = {'OH': 'Ohio', 'KY': 'Kentucky', 'AS': 'American Samoa', 'NV': 'Nevada', 'WY': 'Wyoming', 'NA': 'National', 'AL': 'Alabama', 'MD': 'Maryland', 'AK': 'Alaska', 'UT': 'Utah', 'OR': 'Oregon', 'MT': 'Montana', 'IL': 'Illinois', 'TN': 'Tennessee', 'DC': 'District of Columbia', 'VT': 'Vermont', 'ID': 'Idaho', 'AR': 'Arkansas', 'ME': 'Maine', 'WA': 'Washington', 'HI': 'Hawaii', 'WI': 'Wisconsin', 'MI': 'Michigan', 'IN': 'Indiana', 'NJ': 'New Jersey', 'AZ': 'Arizona', 'GU': 'Guam', 'MS': 'Mississippi', 'PR': 'Puerto Rico', 'NC': 'North Carolina', 'TX': 'Texas', 'SD': 'South Dakota', 'MP': 'Northern Mariana Islands', 'IA': 'Iowa', 'MO': 'Missouri', 'CT': 'Connecticut', 'WV': 'West Virginia', 'SC': 'South Carolina', 'LA': 'Louisiana', 'KS': 'Kansas', 'NY': 'New York', 'NE': 'Nebraska', 'OK': 'Oklahoma', 'FL': 'Florida', 'CA': 'California', 'CO': 'Colorado', 'PA': 'Pennsylvania', 'DE': 'Delaware', 'NM': 'New Mexico', 'RI': 'Rhode Island', 'MN': 'Minnesota', 'VI': 'Virgin Islands', 'NH': 'New Hampshire', 'MA': 'Massachusetts', 'GA': 'Georgia', 'ND': 'North Dakota', 'VA': 'Virginia'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_university_towns():\n",
    "    '''Returns a DataFrame of towns and the states they are in from the \n",
    "    university_towns.txt list. The format of the DataFrame should be:\n",
    "    DataFrame( [ [\"Michigan\", \"Ann Arbor\"], [\"Michigan\", \"Yipsilanti\"] ], \n",
    "    columns=[\"State\", \"RegionName\"]  )\n",
    "    \n",
    "    The following cleaning needs to be done:\n",
    "\n",
    "    1. For \"State\", removing characters from \"[\" to the end.\n",
    "    2. For \"RegionName\", when applicable, removing every character from \" (\" to the end.\n",
    "    3. Depending on how you read the data, you may need to remove newline character '\\n'. '''\n",
    "    df = pd.read_csv('university_towns.txt', header=None, sep='\\t')\n",
    "    df.columns = ['combined']\n",
    "    df['combined'] = df.apply(lambda x: x['combined']+'\\t' if '[edit]' in x['combined'] else '\\t'+x['combined'],\n",
    "                              axis=1)\n",
    "    df['State'] = df['combined'].str.split(\"\\t\").str[0] \\\n",
    "                                .str.split('[').str[0]\n",
    "    df['RegionName'] = df['combined'].str.split(\"\\t\").str[1] \\\n",
    "                                     .str.split(' \\(').str[0]\n",
    "    df = df.applymap(lambda x: np.nan if (x.isspace() or x=='') else x)\n",
    "    df['State'] = df['State'].fillna(method='ffill')\n",
    "    df = df[df['RegionName'].notnull()].drop(['combined'], axis=1)\n",
    "   \n",
    "    \n",
    "    return df\n",
    "\n",
    "df = get_list_of_university_towns()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2008q3'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_recession_start():\n",
    "    '''Returns the year and quarter of the recession start time as a \n",
    "    string value in a format such as 2005q3'''\n",
    "    df = pd.read_excel('gdplev.xls', skiprows=7)\n",
    "    df = df.iloc[:,4:7]\n",
    "    df.columns = ['Quarter', 'GDP in billions of current dollars', 'GDP in billions of chained 2009 dollars']\n",
    "    df = df.set_index('Quarter')\n",
    "    df = df[df.index > '2000']\n",
    "    recession = []\n",
    "    for i in range (np.size(df['GDP in billions of current dollars']) - 2):\n",
    "        if ((df['GDP in billions of current dollars'].iloc[i+1] - df['GDP in billions of current dollars'].iloc[i]) < 0 and\n",
    "            (df['GDP in billions of current dollars'].iloc[i+2] - df['GDP in billions of current dollars'].iloc[i+1]) < 0):\n",
    "            recession += [df.iloc[i].name]\n",
    "    return recession[0]\n",
    "\n",
    "get_recession_start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2009q4'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_recession_end():\n",
    "    '''Returns the year and quarter of the recession end time as a \n",
    "    string value in a format such as 2005q3'''\n",
    "    df = pd.read_excel('gdplev.xls', skiprows=7)\n",
    "    df = df.iloc[:,4:7]\n",
    "    df.columns = ['Quarter', 'GDP in billions of current dollars', 'GDP in billions of chained 2009 dollars']\n",
    "    df = df.set_index('Quarter')\n",
    "    df = df[df.index >= get_recession_start()]\n",
    "    non_recession = []\n",
    "    for i in range (np.size(df['GDP in billions of current dollars']) - 2):\n",
    "        if ((df['GDP in billions of current dollars'].iloc[i+1] - df['GDP in billions of current dollars'].iloc[i]) > 0 and\n",
    "            (df['GDP in billions of current dollars'].iloc[i+2] - df['GDP in billions of current dollars'].iloc[i+1]) > 0):\n",
    "            non_recession += [df.iloc[i+2].name]\n",
    "    return non_recession[0]   \n",
    "\n",
    "\n",
    "get_recession_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2009q2'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_recession_bottom():\n",
    "    df = pd.read_excel('gdplev.xls', skiprows=7)\n",
    "    df = df.iloc[:,4:7]\n",
    "    df.columns = ['Quarter', 'GDP in billions of current dollars', 'GDP in billions of chained 2009 dollars']\n",
    "    df = df.set_index('Quarter')\n",
    "    df = df[(df.index >= get_recession_start()) &\n",
    "            (df.index <= get_recession_end())]\n",
    "    min_gdp = np.min(df['GDP in billions of current dollars'])\n",
    "    df_m_g = df[df['GDP in billions of current dollars']==min_gdp]\n",
    "    return df_m_g.iloc[0].name\n",
    "\n",
    "get_recession_bottom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_housing_data_to_quarters():\n",
    "    '''Converts the housing data to quarters and returns it as mean \n",
    "    values in a dataframe. This dataframe should be a dataframe with\n",
    "    columns for 2000q1 through 2016q3, and should have a multi-index\n",
    "    in the shape of [\"State\",\"RegionName\"].\n",
    "    \n",
    "    Note: Quarters are defined in the assignment description, they are\n",
    "    not arbitrary three month periods.\n",
    "    \n",
    "    The resulting dataframe should have 67 columns, and 10,730 rows.\n",
    "    '''\n",
    "    df = pd.read_csv('City_Zhvi_AllHomes.csv', header=0, sep=',')\n",
    "    df['State'] = df['State'].map(lambda x: states[x])\n",
    "    df = df.set_index(['State', 'RegionName'])\n",
    "    df = df.loc[:,'2000-01':]\n",
    "    df = df.T\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df = df.resample('Q').mean()\n",
    "    df.index = df.index.to_period(\"Q\")\n",
    "    df = df.T\n",
    "    return df\n",
    "\n",
    "\n",
    "df  = convert_housing_data_to_quarters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jwharton\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "def run_ttest():\n",
    "    '''First creates new data showing the decline or growth of housing prices\n",
    "    between the recession start and the recession bottom. Then runs a ttest\n",
    "    comparing the university town values to the non-university towns values, \n",
    "    return whether the alternative hypothesis (that the two groups are the same)\n",
    "    is true or not as well as the p-value of the confidence. \n",
    "    \n",
    "    Return the tuple (different, p, better) where different=True if the t-test is\n",
    "    True at a p<0.01 (we reject the null hypothesis), or different=False if \n",
    "    otherwise (we cannot reject the null hypothesis). The variable p should\n",
    "    be equal to the exact p value returned from scipy.stats.ttest_ind(). The\n",
    "    value for better should be either \"university town\" or \"non-university town\"\n",
    "    depending on which has a lower mean price ratio (which is equivilent to a\n",
    "    reduced market loss).'''\n",
    "    df = convert_housing_data_to_quarters()\n",
    "    recession_start = get_recession_start()\n",
    "    recession_bottom = df.columns.get_loc(get_recession_bottom())\n",
    "    start_m1 = df.columns.get_loc(recession_start) - 1\n",
    "    df2 = df.iloc[:,[start_m1, recession_bottom]]\n",
    "    df2.columns = df2.columns.astype(str)\n",
    "    df2['ratio'] = df2.apply(lambda x: x[0] / x[1] if x[1] != np.nan else np.nan, axis=1)\n",
    "    #ratio_df = pd.DataFrame({'ratio':ratio})\n",
    "    \n",
    "    unis = get_list_of_university_towns()\n",
    "    unis['ut'] = 1\n",
    "    unis = unis.set_index(['State', 'RegionName'])\n",
    "    rat_comb = pd.merge(df2, unis, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    uni_town = rat_comb[rat_comb['ut'] == 1]\n",
    "    nonuni_town = rat_comb[rat_comb['ut'] != 1]\n",
    "    p =      ttest_ind(nonuni_town['ratio'], \n",
    "                       uni_town['ratio'], nan_policy='omit')[1]\n",
    "    if p < 0.01:\n",
    "        different = True\n",
    "    else:\n",
    "        different = False\n",
    "    if np.mean(rat_comb[rat_comb['ut'] == 1].loc[:,'ratio']) > np.mean(rat_comb[rat_comb['ut'] != 1].loc[:,'ratio']):\n",
    "        better = \"university town\"\n",
    "    else:\n",
    "        better = \"non-university town\"\n",
    "    return (different, p, better)\n",
    "    #return uni_town\n",
    "\n",
    "tt = run_ttest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 0.002724063704753125, 'non-university town')"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State                 RegionName         \n",
       "New York              New York               1.081789\n",
       "California            Los Angeles            1.213498\n",
       "Illinois              Chicago                1.082840\n",
       "Pennsylvania          Philadelphia           1.016930\n",
       "Arizona               Phoenix                1.220131\n",
       "Nevada                Las Vegas              1.413590\n",
       "California            San Diego              1.133248\n",
       "Texas                 Dallas                 1.097685\n",
       "California            San Jose               1.153561\n",
       "Florida               Jacksonville           1.139172\n",
       "California            San Francisco          1.127530\n",
       "Texas                 Austin                 1.047712\n",
       "Michigan              Detroit                1.141986\n",
       "Ohio                  Columbus               1.034012\n",
       "Tennessee             Memphis                1.110526\n",
       "North Carolina        Charlotte              1.065184\n",
       "Texas                 El Paso                0.992137\n",
       "Massachusetts         Boston                 1.070008\n",
       "Washington            Seattle                1.131263\n",
       "Maryland              Baltimore              1.032188\n",
       "Colorado              Denver                 1.050454\n",
       "District of Columbia  Washington             1.077943\n",
       "Tennessee             Nashville              1.029751\n",
       "Wisconsin             Milwaukee              1.124215\n",
       "Arizona               Tucson                 1.098763\n",
       "Oregon                Portland               1.039554\n",
       "Oklahoma              Oklahoma City          1.019919\n",
       "Nebraska              Omaha                  1.016904\n",
       "New Mexico            Albuquerque            0.996758\n",
       "California            Fresno                 1.264370\n",
       "                                               ...   \n",
       "Texas                 Granite Shoals              NaN\n",
       "Maryland              Piney Point            1.076788\n",
       "Wisconsin             Maribel                     NaN\n",
       "Idaho                 Middleton              1.134488\n",
       "Colorado              Bennett                1.066555\n",
       "New Hampshire         East Hampstead         1.101909\n",
       "Missouri              Garden City                 NaN\n",
       "Arkansas              Mountainburg           0.980575\n",
       "Wisconsin             Oostburg               1.062945\n",
       "California            Twin Peaks             1.182234\n",
       "New York              Upper Brookville       1.074505\n",
       "Hawaii                Volcano                1.216234\n",
       "South Carolina        Wedgefield                  NaN\n",
       "Michigan              Williamston            1.172748\n",
       "Arkansas              Decatur                0.977849\n",
       "Tennessee             Briceville             1.072191\n",
       "Indiana               Edgewood               0.933566\n",
       "Tennessee             Palmyra                     NaN\n",
       "Maryland              Saint Inigoes          1.089441\n",
       "Indiana               Marysville                  NaN\n",
       "California            Forest Falls           1.203214\n",
       "Missouri              Bois D Arc             0.890094\n",
       "Virginia              Henrico                1.053978\n",
       "New Jersey            Diamond Beach          1.087641\n",
       "Tennessee             Gruetli Laager         0.732081\n",
       "Wisconsin             Town of Wrightstown    1.035443\n",
       "New York              Urbana                 0.938968\n",
       "Wisconsin             New Denmark            0.969991\n",
       "California            Angels                 1.159675\n",
       "Wisconsin             Holland                1.135721\n",
       "Length: 10730, dtype: float64"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.apply(lambda x: x[0] / x[1] if x[1] != np.nan else np.nan, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['State', 'RegionName'], dtype='object')"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 0.002724063704753125, 'non-university town')"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_ttest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2008q3'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recession_start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2009q2'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recession_bottom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jwharton\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "df = convert_housing_data_to_quarters()\n",
    "recession_start = get_recession_start()\n",
    "recession_bottom = df.columns.get_loc(get_recession_bottom())\n",
    "start_m1 = df.columns.get_loc(recession_start) - 1\n",
    "df2 = df.iloc[:,[start_m1, recession_bottom]]\n",
    "df2.columns = df2.columns.astype(str)\n",
    "df2['ratio'] = df2.apply(lambda x: x[0] / x[1] if x[1] != np.nan else np.nan, axis=1)\n",
    "#ratio_df = pd.DataFrame({'ratio':ratio})\n",
    "   \n",
    "unis = get_list_of_university_towns()\n",
    "unis['ut'] = 1\n",
    "unis = unis.set_index(['State', 'RegionName'])\n",
    "rat_comb = pd.merge(df2, unis, how='left', left_index=True, right_index=True)\n",
    "   \n",
    "uni_town = rat_comb[rat_comb['ut'] == 1]\n",
    "nonuni_town = rat_comb[rat_comb['ut'] != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=2.998032664179151, pvalue=0.002724063704753125)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_ind(nonuni_town['ratio'], uni_town['ratio'], nan_policy='omit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2008Q2    9599\n",
       "2009Q2    9671\n",
       "ratio     9599\n",
       "ut           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonuni_town.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10461, 4)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonuni_town.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10730, 67)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(keep=False).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(517, 1)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unis.drop_duplicates(keep=False).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(517, 3)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unis.reset_index().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(517, 3)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unis.reset_index().drop_duplicates(keep=False).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
